{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11984c49-f615-447e-a6b0-738196d288ec",
   "metadata": {},
   "source": [
    "## CLUSTERING:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba374a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IMPORT MODULES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22b950a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/deniz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/deniz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import mpld3\n",
    "import codecs\n",
    "import collections\n",
    "\n",
    "import math as mh\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string as st\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from itertools import chain\n",
    "from itertools import islice\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import feature_extraction\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca9b7cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### READ CSVS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "069170ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/vkusvill_items.csv\", 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    catalog_df = pd.DataFrame(reader)\n",
    "\n",
    "catalog_df = catalog_df.rename(columns = {\"\": \"shop_id\", \"item_composition_txt\": \"item_composition\", \"nutrion_value_txt\" : \"nutrion_value\"})\n",
    "catalog = catalog_df.drop([\"shop_id\", \"price\", \"vat\", \"measure_unit\", \"measure_value\", \"protein_value\", \"fat_value\", \"carb_value\", \n",
    "                           \"energy_value\", \"measure_quantum\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be6fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_df = pd.read_csv(\"../data/vkusvill_categories.csv\")\n",
    "categories_df = categories_df.rename( columns = {\"Unnamed: 0\": \"shop_id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71370f-a000-4822-a5f8-be2591df67cd",
   "metadata": {},
   "source": [
    "### Prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1600478-cda6-4359-9986-81e391844b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_categorie(df, categorie):\n",
    "    result = pd.DataFrame();\n",
    "    \n",
    "    for line in range(len(df.index)):\n",
    "        if str(categorie) in df[\"categories_array\"][line]:\n",
    "            df_string = pd.DataFrame({\"item_id\": [df[\"item_id\"][line]], \"categories_array\": [df[\"categories_array\"][line]],\n",
    "                                     \"item_name\": [df[\"item_name\"][line]], \"item_composition\": [df[\"item_composition\"][line]],\n",
    "                                     \"nutrion_value\": [df[\"nutrion_value\"][line]]})\n",
    "            \n",
    "            result = pd.concat([result, df_string], ignore_index=True)\n",
    "\n",
    "    return result\n",
    "    \n",
    "result = check_categorie(catalog, 13152)\n",
    "result = result.drop([\"categories_array\", \"item_name\", \"nutrion_value\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99bf01b3-6b43-4f6a-b0a3-a8972f52d287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_composition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43499</td>\n",
       "      <td>мясо кролика (50%), мясо индейки (50%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61936</td>\n",
       "      <td>мясо кур, мясо индеек, мясо кролика.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62798</td>\n",
       "      <td>говядина</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62804</td>\n",
       "      <td>Телятина\\nПродукция производится на предприяти...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62806</td>\n",
       "      <td>Свинина, говядина</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>62809</td>\n",
       "      <td>Говядина\\nПродукция производится на предприяти...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69576</td>\n",
       "      <td>Свинина (50%), говядина (50%)\\nПроизводится на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71029</td>\n",
       "      <td>Говядина, филе грудки куриной\\nПродукция произ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20336</td>\n",
       "      <td>мясо кролика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21194</td>\n",
       "      <td>ЯРОСЛАВСКИЙ БРОЙЛЕР АО: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>21626</td>\n",
       "      <td>мясо цыплят-бройлеров.\\nПродукция производится...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>37018</td>\n",
       "      <td>Мясо индейки. Продукт производится на предприя...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>37321</td>\n",
       "      <td>свинина</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>51061</td>\n",
       "      <td>Свинина, говядина</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>51062</td>\n",
       "      <td>Говядина</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>60812</td>\n",
       "      <td>Индейка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>61014</td>\n",
       "      <td>курица</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51065</td>\n",
       "      <td>свинина</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>62895</td>\n",
       "      <td>Телятина</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>67563</td>\n",
       "      <td>Свинина. Говядина</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>67810</td>\n",
       "      <td>Филе грудки индейки</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>67811</td>\n",
       "      <td>Филе грудки куриной</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>67547</td>\n",
       "      <td>говядина</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>67548</td>\n",
       "      <td>Свинина</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                                   item_composition\n",
       "0    43499             мясо кролика (50%), мясо индейки (50%)\n",
       "1    61936               мясо кур, мясо индеек, мясо кролика.\n",
       "2    62798                                           говядина\n",
       "3    62804  Телятина\\nПродукция производится на предприяти...\n",
       "4    62806                                  Свинина, говядина\n",
       "5    62809  Говядина\\nПродукция производится на предприяти...\n",
       "6    69576  Свинина (50%), говядина (50%)\\nПроизводится на...\n",
       "7    71029  Говядина, филе грудки куриной\\nПродукция произ...\n",
       "8    20336                                       мясо кролика\n",
       "9    21194  ЯРОСЛАВСКИЙ БРОЙЛЕР АО: мясо цыплят-бройлеров ...\n",
       "10   21626  мясо цыплят-бройлеров.\\nПродукция производится...\n",
       "11   37018  Мясо индейки. Продукт производится на предприя...\n",
       "12   37321                                            свинина\n",
       "13   51061                                  Свинина, говядина\n",
       "14   51062                                           Говядина\n",
       "15   60812                                            Индейка\n",
       "16   61014                                             курица\n",
       "17   51065                                            свинина\n",
       "18   62895                                           Телятина\n",
       "19   67563                                  Свинина. Говядина\n",
       "20   67810                               Филе грудки индейки \n",
       "21   67811                               Филе грудки куриной \n",
       "22   67547                                           говядина\n",
       "23   67548                                            Свинина"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f516651",
   "metadata": {},
   "source": [
    "### PAIRS ITEM NAMES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bbb77f3-9b36-4804-9a9b-81e71de2b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_item_name(item_name):\n",
    "    answer = \"\"\n",
    "    \n",
    "    for symbol in item_name:\n",
    "        if symbol == '_':\n",
    "            answer += ' '\n",
    "            continue  \n",
    "        elif symbol in [\n",
    "            '\\\\', \n",
    "            '(', \n",
    "            ')', \n",
    "            '.', \n",
    "            ',', \n",
    "            '%'\n",
    "        ] or symbol.isdigit():\n",
    "            answer += ' '\n",
    "            continue\n",
    "        else:\n",
    "            answer += symbol\n",
    "\n",
    "    answer = answer.replace(\" шт\", ' ')\n",
    "    answer = answer.replace(\" мл\", ' ')\n",
    "    answer = answer.replace(\" мг\", ' ')\n",
    "    answer = answer.replace(\" мг\", ' ')\n",
    "    answer = answer.replace(\" премиум\", ' ')\n",
    "    answer = answer.replace(\" вес\", ' ')\n",
    "    answer = answer.replace(\" из \", ' ')\n",
    "    answer = answer.replace(\" спб \", ' ')\n",
    "    answer = answer.replace(\" по \", ' ')\n",
    "    answer = answer.replace(\" вкусвилл \", ' ')\n",
    "    answer = answer.replace(\" стандарту \", ' ')\n",
    "    answer = answer.replace(\" вв\", ' ')\n",
    "    answer = answer.replace(\" и \", ' ')\n",
    "    answer = answer.replace(\" постный \", ' ')\n",
    "    answer = answer.replace(\" мяса \", ' ')\n",
    "    answer = answer.replace(\" охлажденный\", ' ')\n",
    "    answer = answer.replace(\" филе\", ' ')\n",
    "    \n",
    "    while \"  \" in answer:\n",
    "        answer = answer.replace(\"  \", ' ')\n",
    "    \n",
    "    return answer.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29cab2c1-b8bf-49e6-9680-fefb5e8876d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_catalog_df = pd.Series()\n",
    "\n",
    "for i in range(len(result.index)):\n",
    "    for i in range(10):\n",
    "        tmp = clear_item_name(result[\"item_composition\"][i])\n",
    "        \n",
    "    prepare_catalog_df = pd.concat([prepare_catalog_df, pd.Series(tmp[:-1])], ignore_index=True)\n",
    "\n",
    "prepare_catalog_df\n",
    "\n",
    "final_result = pd.concat([result, prepare_catalog_df], ignore_index=True, axis=1)\n",
    "final_result = final_result.drop([1], axis=1)\n",
    "final_result = final_result.rename( columns = {0: \"item_id\", 2: \"clear_composition\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec329bf6-fd16-4fde-b014-19af7912848c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>clear_composition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43499</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61936</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62798</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62804</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62806</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>62809</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69576</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71029</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20336</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21194</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>21626</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>37018</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>37321</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>51061</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>51062</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>60812</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>61014</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51065</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>62895</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>67563</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>67810</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>67811</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>67547</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>67548</td>\n",
       "      <td>ярославский бройлер ао: мясо цыплят-бройлеров ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                                  clear_composition\n",
       "0    43499  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "1    61936  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "2    62798  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "3    62804  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "4    62806  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "5    62809  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "6    69576  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "7    71029  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "8    20336  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "9    21194  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "10   21626  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "11   37018  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "12   37321  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "13   51061  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "14   51062  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "15   60812  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "16   61014  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "17   51065  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "18   62895  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "19   67563  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "20   67810  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "21   67811  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "22   67547  ярославский бройлер ао: мясо цыплят-бройлеров ...\n",
       "23   67548  ярославский бройлер ао: мясо цыплят-бройлеров ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087e8e2-2477-43cd-94ee-ea12f00b87f5",
   "metadata": {},
   "source": [
    "### CLUSTERING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d714a6d4-e978-4548-a4c2-95468cca1304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 запросов считано\n"
     ]
    }
   ],
   "source": [
    "print(str(len(final_result)) + ' запросов считано')\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "def token_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if re.search('[а-яА-Я]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def token_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if re.search('[а-яА-Я]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "#Создаем словари (массивы) из полученных основ\n",
    "totalvocab_stem = []\n",
    "totalvocab_token = []\n",
    "\n",
    "for i in range(len(result.index)):\n",
    "    allwords_stemmed = token_and_stem(final_result[\"clear_composition\"][i])\n",
    "    # print(allwords_stemmed)\n",
    "    totalvocab_stem.extend(allwords_stemmed)    \n",
    "    allwords_tokenized = token_only(final_result[\"clear_composition\"][i])\n",
    "    totalvocab_token.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09831337-07f2-4842-b132-3f47a82cd4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deniz/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/deniz/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['бол', 'больш', 'будт', 'быт', 'вед', 'впроч', 'всег', 'всегд', 'даж', 'друг', 'е', 'ег', 'ем', 'есл', 'ест', 'ещ', 'зач', 'зде', 'ил', 'иногд', 'когд', 'конечн', 'куд', 'лучш', 'межд', 'мен', 'мног', 'мо', 'можн', 'нег', 'нельз', 'нибуд', 'никогд', 'нич', 'опя', 'посл', 'пот', 'почт', 'разв', 'сво', 'себ', 'совс', 'теб', 'тепер', 'тог', 'тогд', 'тож', 'тольк', 'хорош', 'хот', 'чег', 'чут', 'эт'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m n_featur\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000000\u001b[39m\n\u001b[1;32m      7\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                  min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39mstopwords,\n\u001b[1;32m      9\u001b[0m                                  use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tokenizer\u001b[38;5;241m=\u001b[39mtoken_and_stem, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclear_composition\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mmagic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime tfidf_matrix = tfidf_vectorizer.fit_transform(final_result[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclear_composition\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m])\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(tfidf_matrix\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2125\u001b[0m )\n\u001b[0;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1396\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1395\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[0;32m-> 1396\u001b[0m X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_words_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_limit_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1400\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1248\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m   1246\u001b[0m kept_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kept_indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter pruning, no terms remain. Try a lower min_df or a higher max_df.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1250\u001b[0m     )\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X[:, kept_indices], removed_terms\n",
      "\u001b[0;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('russian')\n",
    "#можно расширить список стоп-слов\n",
    "stopwords.extend(['как', 'в', 'к', 'на', \"из\", \"по\", 'г', \"вв\", \"вкуссвилл\", \"вес\", \"спб\"])\n",
    "\n",
    "n_featur=2000000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, max_features=10000,\n",
    "                                 min_df=0.15, stop_words=stopwords,\n",
    "                                 use_idf=True, tokenizer=token_and_stem, ngram_range=(1,3))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(final_result[\"clear_composition\"])\n",
    "get_ipython().magic('time tfidf_matrix = tfidf_vectorizer.fit_transform(final_result[\"clear_composition\"])')\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbca983-00f2-4904-8765-bed30f7e8dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 20\n",
    "\n",
    "# Метод к-средних - KMeans\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "get_ipython().magic('time km.fit(tfidf_matrix)')\n",
    "idx = km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# # MiniBatchKMeans\n",
    "# from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# mbk  = MiniBatchKMeans(init='random', n_clusters=num_clusters) #(init='k-means++', ‘random’ or an ndarray)\n",
    "# mbk.fit_transform(tfidf_matrix)\n",
    "# %time mbk.fit(tfidf_matrix)\n",
    "# miniclusters = mbk.labels_.tolist()\n",
    "# print (mbk.labels_)\n",
    "\n",
    "# # DBSCAN\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# get_ipython().magic('time db = DBSCAN(eps=0.3, min_samples=10).fit(tfidf_matrix)')\n",
    "# labels = db.labels_\n",
    "# labels.shape\n",
    "# print(labels)\n",
    "\n",
    "# # Аггломеративная класстеризация\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# agglo1 = AgglomerativeClustering(n_clusters=num_clusters, affinity='euclidean') #affinity можно выбрать любое или попробовать все по очереди: cosine, l1, l2, manhattan\n",
    "# get_ipython().magic('time answer = agglo1.fit_predict(tfidf_matrix.toarray())')\n",
    "# answer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44007cf-bb15-43c7-9131-824d6b6b169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clusters)\n",
    "print (km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56f67b-af69-4b1e-ae0d-f7247dcb7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means\n",
    "clusterkm = km.labels_.tolist()\n",
    "# #minikmeans\n",
    "# clustermbk = mbk.labels_.tolist()\n",
    "# #dbscan\n",
    "# clusters3 = labels\n",
    "#  #agglo\n",
    "# #clusters4 = answer.tolist()\n",
    "\n",
    "frame = pd.DataFrame(final_result[\"clear_name\"], index = [clusterkm])\n",
    "\n",
    "#k-means\n",
    "out = { 'title': range(len(result.index)), 'cluster': clusterkm }\n",
    "frame1 = pd.DataFrame(out, index = [clusterkm], columns = ['title', 'cluster'])\n",
    "frame1 = frame1.reset_index(drop=True)\n",
    "\n",
    "# #mini\n",
    "# out = { 'title': final_result[\"clear_name\"], 'cluster': clustermbk }\n",
    "# frame_minik = pd.DataFrame(out, index = [clustermbk], columns = ['title', 'cluster'])\n",
    "\n",
    "# frame1['cluster'].value_counts()\n",
    "\n",
    "dict = {}\n",
    "\n",
    "for i in range(len(frame1.index)):\n",
    "    dict[i] = frame1['cluster'][i]   \n",
    "\n",
    "dict = pd.Series(dict)\n",
    "dict\n",
    "\n",
    "final = pd.concat([final_result, dict], ignore_index= True, axis=1)\n",
    "\n",
    "final = final.rename(columns = {0: \"item_id\", 1: \"categories\", 2: \"composition\", 3: \"nutriotion\", 4: \"clear_name\", 5: \"cluster\"})\n",
    "# result\n",
    "# frame_minik['cluster'].value_counts()\n",
    "final = final.sort_values(by='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe074c9-c53d-4aa8-8d31-0e009ebff38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
